{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 数据预处理 ========================\n",
    "\n",
    "def data_preprocessing(line):\n",
    "    line = line.replace('<br /><br />','')\n",
    "    line = line + ' '\n",
    "    change_word = ['.', '!', ',' , ':', '?', '(', ')', '/']\n",
    "    for word in change_word:\n",
    "        line = line.replace(word, ' '+word+' ')\n",
    "    line = line.replace('  ',' ')\n",
    "    line = line + '\\n'\n",
    "    return line\n",
    "\n",
    "f=open('GloVe-master/corpus.txt','w')\n",
    "\n",
    "#获得训练集文件夹\n",
    "filedir_pos = 'aclImdb/train/pos/'\n",
    "filedir_neg = 'aclImdb/train/neg/'\n",
    "\n",
    "#获取当前文件夹中的文件名称列表\n",
    "filenames_pos = os.listdir(filedir_pos)\n",
    "filenames_neg = os.listdir(filedir_neg)\n",
    "train_list, train_labels = [], []\n",
    "for filename in filenames_pos:\n",
    "    filepath = filedir_pos + filename\n",
    "    for line in open(filepath):\n",
    "        line = data_preprocessing(line)\n",
    "        f.write(line)\n",
    "        train_list.append(line)\n",
    "        train_labels.append([1])\n",
    "for filename in filenames_neg:\n",
    "    filepath = filedir_neg + filename\n",
    "    for line in open(filepath):\n",
    "        line = data_preprocessing(line)\n",
    "        f.write(line)\n",
    "        train_list.append(line)\n",
    "        train_labels.append([0])\n",
    "f.close()\n",
    "\n",
    "#获得验证集文件夹\n",
    "filedir_pos = 'aclImdb/test/pos/'\n",
    "filedir_neg = 'aclImdb/test/neg/'\n",
    "\n",
    "#获取当前文件夹中的文件名称列表\n",
    "filenames_pos = os.listdir(filedir_pos)\n",
    "filenames_neg = os.listdir(filedir_neg)\n",
    "test_list, test_labels = [], []\n",
    "for filename in filenames_pos:\n",
    "    filepath = filedir_pos + filename\n",
    "    for line in open(filepath):\n",
    "        test_list.append(data_preprocessing(line))\n",
    "        test_labels.append([1])   \n",
    "for filename in filenames_neg:\n",
    "    filepath = filedir_neg + filename\n",
    "    for line in open(filepath):\n",
    "        test_list.append(data_preprocessing(line))\n",
    "        test_labels.append([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 将GloVe处理成Word2Vec ========================\n",
    "\n",
    "# glove_file = './GloVe-master/vectors.txt'\n",
    "# tmp_file = './GloVe-master/word2vec_model.txt'\n",
    "# _ = glove2word2vec(glove_file, tmp_file)\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./GloVe-master/word2vec_model.txt',binary=False, encoding='utf-8')\n",
    "vocab_list = list(w2v_model.vocab.keys())\n",
    "#print(len(vocab_list)) #37125\n",
    "word_index = {word: index for index, word in enumerate(vocab_list)}  #获得字典：{'the': 0, 'a': 1...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dictionary, sample_list, sample_labels):\n",
    "    sample_features = []                                                \n",
    "    for sentence in sample_list:\n",
    "        words = []\n",
    "        sentence = sentence.split(\" \")\n",
    "        for word in sentence:\n",
    "            if word not in dictionary:\n",
    "                words.append(0)\n",
    "            else:\n",
    "                words.append(dictionary[word])\n",
    "        sample_features.append(words)                                   \n",
    "\n",
    "    sample_data = []                                                    \n",
    "    for i in range(len(sample_features)):\n",
    "        temp = []\n",
    "        temp.append(torch.Tensor(sample_features[i]).long())\n",
    "        temp.append(torch.Tensor(sample_labels[i]).long())\n",
    "        sample_data.append(temp)\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "train_data = get_dataset(word_index, train_list, train_labels)\n",
    "test_data = get_dataset(word_index, test_list, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embeding_vector, kernel_sizes, num_channels):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        #不参与训练的嵌入层\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embeding_vector))  #使用预训练的词向量\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        #参与训练的嵌入层\n",
    "        self.constant_embedding = torch.nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size)\n",
    "        self.constant_embedding.weight.data.copy_(torch.from_numpy(embeding_vector))  #使用预训练的词向量\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.out_linear = torch.nn.Linear(sum(num_channels), output_size)\n",
    "        self.pool = GlobalMaxPool1d()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(torch.nn.Conv1d(in_channels=2*hidden_size, out_channels=c, kernel_size=k))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = torch.cat((self.embedding(x), self.constant_embedding(x)), dim=2).permute(0,2,1)\n",
    "        out = torch.cat([self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        out = self.out_linear(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "class GlobalMaxPool1d(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.max_pool1d(x, kernel_size = x.shape[2])\n",
    "\n",
    "\n",
    "def collate_fn(sample_data):\n",
    "    sample_data.sort(key=lambda data: len(data[0]), reverse=True)                          #倒序排序\n",
    "    sample_features = []\n",
    "    sample_labels = []\n",
    "    for data in sample_data:\n",
    "        sample_features.append(data[0])\n",
    "        sample_labels.append(data[1])\n",
    "    data_length = [len(data[0]) for data in sample_data]                                   #取出所有data的长度             \n",
    "    sample_features = rnn_utils.pad_sequence(sample_features, batch_first=True, padding_value=0) \n",
    "    return sample_features, sample_labels, data_length\n",
    "\n",
    "def test_evaluate(model, test_dataloader, batch_size, num_epoch):\n",
    "    test_l, test_a, n = 0.0, 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data_x, data_y, _ in test_dataloader:\n",
    "            label = torch.Tensor(data_y).long().to(device)\n",
    "            out = model(data_x.to(device))\n",
    "            prediction = out.argmax(dim=1)\n",
    "            loss = loss_func(out, label)\n",
    "            prediction = out.argmax(dim=1).data.cpu().numpy()\n",
    "            label = label.data.cpu().numpy()\n",
    "            test_l += loss.item()\n",
    "            test_a += accuracy_score(label, prediction)\n",
    "            n += 1\n",
    "    return test_l/n, test_a/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model...\n",
      "epoch 1, train_loss 0.165267, train_accuracy 0.948237, test_loss 0.302531, test_accuracy 0.873639, time 0:00:10.858645\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3674d6f9c275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 让Embedding层使用训练好的Word2Vec权重\n",
    "embedding_matrix = w2v_model.vectors\n",
    "input_size = embedding_matrix.shape[0]   #37125, 词典的大小\n",
    "hidden_size = embedding_matrix.shape[1]  #50, 隐藏层单元个数\n",
    "kernel_size = [3, 4, 5]\n",
    "nums_channels = [100, 100, 100]\n",
    "model = TextCNN(input_size, hidden_size, 2, embedding_matrix, kernel_size, nums_channels).to(device)\n",
    "model.load_state_dict(torch.load('./model_save/TextCNN_save_2.pt'))\n",
    "print(\"load model...\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0000001, weight_decay=0.1)#\n",
    "\n",
    "batch_size = 256\n",
    "num_epoch = 50\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "loss_min = 0.302007\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    train_l, train_a, n = 0.0, 0.0, 0\n",
    "    start = datetime.datetime.now()\n",
    "    for data_x, data_y, _ in train_dataloader:\n",
    "        label = torch.Tensor(data_y).long().to(device)\n",
    "        out = model(data_x.to(device))\n",
    "        loss = loss_func(out, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prediction = out.argmax(dim=1).data.cpu().numpy()\n",
    "        label = label.data.cpu().numpy()\n",
    "        train_l += loss.item()\n",
    "        train_a += accuracy_score(label, prediction)\n",
    "        n += 1\n",
    "    #训练集评价指标\n",
    "    train_loss.append(train_l/n)\n",
    "    train_accuracy.append(train_a/n)\n",
    "    #测试集评价指标\n",
    "    test_l, test_a = test_evaluate(model, test_dataloader, batch_size, num_epoch)\n",
    "    test_loss.append(test_l)\n",
    "    test_accuracy.append(test_a)\n",
    "    end = datetime.datetime.now()\n",
    "    print('epoch %d, train_loss %f, train_accuracy %f, test_loss %f, test_accuracy %f, time %s'% \n",
    "          (epoch+1, train_loss[epoch], train_accuracy[epoch], test_loss[epoch], test_accuracy[epoch], end-start))\n",
    "    if test_loss[epoch] < loss_min:\n",
    "        loss_min = test_loss[epoch]\n",
    "        torch.save(model.state_dict(), './model_save/TextCNN_save_2.pt')\n",
    "        print(\"save model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
