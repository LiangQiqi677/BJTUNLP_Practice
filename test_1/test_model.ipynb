{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn.functional as F\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./GloVe-master/word2vec_model.txt',binary=False, encoding='utf-8')\n",
    "vocab_list = list(w2v_model.vocab.keys())\n",
    "word_index = {word: index for index, word in enumerate(vocab_list)}  #获得字典：{'the': 0, 'a': 1...}\n",
    "\n",
    "#获得测试集文件夹\n",
    "filepath = './test.txt'\n",
    "\n",
    "test_list = []\n",
    "for line in open(filepath):\n",
    "    line = line.replace('<br /><br />','')\n",
    "    change_word = ['.', '!', ',' , ':', '?', '(', ')', '/']\n",
    "    for word in change_word:\n",
    "        line = line.replace(word, ' '+word+' ')\n",
    "    line = line.replace('  ',' ')\n",
    "    words = []\n",
    "    line = line.split(\" \")\n",
    "    for word in line:\n",
    "        if word not in word_index:\n",
    "            words.append(0)\n",
    "        else:\n",
    "            words.append(word_index[word])\n",
    "        #[words.append(0) for i in range(5-len(words)) if len(words) < 5]         \n",
    "    test_list.append(torch.Tensor(words).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embeding_vector, kernel_sizes, num_channels):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        #不参与训练的嵌入层\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embeding_vector))  #使用预训练的词向量\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        #参与训练的嵌入层\n",
    "        self.constant_embedding = torch.nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size)\n",
    "        self.constant_embedding.weight.data.copy_(torch.from_numpy(embeding_vector))  #使用预训练的词向量\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.out_linear = torch.nn.Linear(sum(num_channels), output_size)\n",
    "        self.pool = GlobalMaxPool1d()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(torch.nn.Conv1d(in_channels=2*hidden_size, out_channels=c, kernel_size=k))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = torch.cat((self.embedding(x), self.constant_embedding(x)), dim=2).permute(0,2,1)\n",
    "        out = torch.cat([self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        out = self.out_linear(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "class GlobalMaxPool1d(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.max_pool1d(x, kernel_size = x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "# 让Embedding层使用训练好的Word2Vec权重\n",
    "embedding_matrix = w2v_model.vectors\n",
    "input_size = embedding_matrix.shape[0]   #49339, 词典的大小\n",
    "hidden_size = embedding_matrix.shape[1]  #50, 隐藏层单元个数\n",
    "kernel_size = [3, 4, 5]\n",
    "nums_channels = [100, 100, 100]\n",
    "model = TextCNN(input_size, hidden_size, 2, embedding_matrix, kernel_size, nums_channels).to(device)\n",
    "model.load_state_dict(torch.load('./model_save/TextCNN_save_2.pt'))\n",
    "\n",
    "f=open('result.txt','w')\n",
    "for data_x in test_list:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data_x.unsqueeze(0).to(device))\n",
    "        prediction = out.argmax(dim=1).data.cpu().numpy()\n",
    "        if prediction[0] == 0:\n",
    "            f.write('0(negative)\\n')\n",
    "        else:\n",
    "            f.write('1(positive)\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
