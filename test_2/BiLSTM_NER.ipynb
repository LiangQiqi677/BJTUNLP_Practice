{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import argparse\n",
    "from gensim.models import Word2Vec\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "parser.add_argument('--num_epoch', type=int, default=10)\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入训练集\n",
    "with open('./CoNLL2003_NER/train/seq.in', encoding='utf-8') as ftrain_feature:\n",
    "    train_feature_line = [line.strip() for line in ftrain_feature.readlines()][:10]\n",
    "with open('./CoNLL2003_NER/train/seq.out', encoding='utf-8') as ftrain_label:\n",
    "    train_label_line = [line.strip() for line in ftrain_label.readlines()][:10]\n",
    "#读入验证集\n",
    "with open('./CoNLL2003_NER/test/seq.in', encoding='utf-8') as ftest_feature:\n",
    "    test_feature_line = [line.strip() for line in ftest_feature.readlines()][:10]\n",
    "with open('./CoNLL2003_NER/test/seq.out', encoding='utf-8') as ftest_label:\n",
    "    test_label_line = [line.strip() for line in ftest_label.readlines()][:10]\n",
    "\n",
    "#转换大小写并用split分隔开存入列表\n",
    "train_feature_line = [line.lower().split(\" \") for line in train_feature_line]\n",
    "train_label_line = [line.split(\" \") for line in train_label_line]\n",
    "test_feature_line = [line.lower().split(\" \") for line in test_feature_line]\n",
    "test_label_line = [line.split(\" \") for line in test_label_line]\n",
    "\n",
    "#获得单词字典\n",
    "word_counter = []\n",
    "for line in train_feature_line:\n",
    "    word_counter.extend(line)\n",
    "word_counter = Counter(word_counter).most_common()                                          #len(counter):21009\n",
    "vocab = ['[UNK]','[PAD]'] + [word[0] for word in word_counter[:int(len(word_counter)*0.8)]] #UNK:低频词；PAD:填充词\n",
    "word2id = dict(zip(vocab,range(len(vocab))))                                                # word -> id\n",
    "id2word = {idx:word for idx,word in enumerate(vocab)}                                       # id -> word\n",
    "#获得标签字典\n",
    "label2id = {'O':0, 'B-LOC':1, 'B-PER':2, 'B-ORG':3, 'I-PER':4, 'I-ORG':5, 'B-MISC':6, 'I-LOC':7, 'I-MISC':8}\n",
    "\n",
    "#获得数据和标签序列\n",
    "train_feature = [[word2id[word] if word in word2id else 0 for word in line] for line in train_feature_line]\n",
    "train_label = [[label2id[word] for word in line] for line in train_label_line]\n",
    "test_feature = [[word2id[word] if word in word2id else 0 for word in line] for line in test_feature_line]\n",
    "test_label = [[label2id[word] for word in line] for line in test_label_line]\n",
    "\n",
    "#转成Tensor的形式\n",
    "train_feature = [torch.Tensor(line).long() for line in train_feature]\n",
    "train_label = [torch.Tensor(line).long() for line in train_label]\n",
    "test_feature = [torch.Tensor(line).long() for line in test_feature]\n",
    "test_label = [torch.Tensor(line).long() for line in test_label]\n",
    "\n",
    "def get_data(sample_features, sample_labels):\n",
    "    sample_data = []                                                    #为了能够将data放到DataLoader中\n",
    "    for i in range(len(sample_features)):\n",
    "        temp = []\n",
    "        temp.append(sample_features[i])\n",
    "        temp.append(sample_labels[i])\n",
    "        sample_data.append(temp)\n",
    "    return sample_data\n",
    "\n",
    "def collate_fn(sample_data):\n",
    "    sample_data.sort(key=lambda data: len(data[0]), reverse=True)                          #倒序排序\n",
    "    sample_features, sample_labels = [], []\n",
    "    for data in sample_data:\n",
    "        sample_features.append(data[0])\n",
    "        sample_labels.append(data[1])\n",
    "    data_length = [len(data[0]) for data in sample_data]                                   #取出所有data的长度             \n",
    "    sample_features = torch.nn.utils.rnn.pad_sequence(sample_features, batch_first=True, padding_value=word2id['[PAD]']) \n",
    "    return sample_features, sample_labels, data_length\n",
    "\n",
    "train_data = get_data(train_feature, train_label)\n",
    "test_data = get_data(test_feature, test_label)\n",
    "\n",
    "#处理非定长序列\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, args.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, args.batch_size, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_vector):\n",
    "        super(BiLSTM_CRF,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_vector))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.bilstm = torch.nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=2, \n",
    "                                    batch_first=True, dropout=0.5, bidirectional=True)\n",
    "        self.out = torch.nn.Linear(2 * hidden_size,output_size)\n",
    "    \n",
    "    def forward(self, x, batch_seq_len):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        h = torch.zeros(4, batch_size, self.hidden_size).to(x.device) \n",
    "        c = torch.zeros(4, batch_size, self.hidden_size).to(x.device) \n",
    "        \n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x,batch_seq_len, batch_first=True)\n",
    "        output, hidden = self.bilstm(x, (h, c))\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output,batch_first=True)\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "#通过batch_seq_len留下out和label中不重复的部分进行loss计算和指标计算\n",
    "def processing_len(out, label, batch_seq_len):\n",
    "    out_pred = out[:batch_seq_len[0],:]\n",
    "    out_true = label[:batch_seq_len[0]]\n",
    "    for i in range(1,len(batch_seq_len)):\n",
    "        out_pred = torch.cat((out_pred,out[i*batch_seq_len[0]:i*batch_seq_len[0]+batch_seq_len[i],:]),dim=0)\n",
    "        out_true = torch.cat((out_true,label[i*batch_seq_len[0]:i*batch_seq_len[0]+batch_seq_len[i]]),dim=0)\n",
    "\n",
    "    return out_pred, out_true\n",
    "    \n",
    "def test_evaluate(model, test_dataloader, batch_size):\n",
    "    test_l, test_p, test_r, test_f, n = 0.0, 0.0, 0.0, 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data_x, data_y, batch_seq_len in test_dataloader:\n",
    "            out = model(data_x.to(device),batch_seq_len).view(-1, 9)\n",
    "            label = [line.numpy().tolist() for line in data_y]\n",
    "            for line in label:\n",
    "                for i in range(data_x.shape[1]-len(line)):\n",
    "                    line.append(line[len(line)-1])\n",
    "            label = torch.tensor(label).view(-1,1).squeeze(-1).to(device)\n",
    "            out, label = processing_len(out, label,batch_seq_len)\n",
    "            prediction = out.argmax(dim=1).data.cpu().numpy()\n",
    "            label = label.data.cpu().numpy()\n",
    "            test_l += loss.item()\n",
    "            test_p += precision_score(label, prediction, average='weighted')\n",
    "            test_r += recall_score(label, prediction, average='weighted')\n",
    "            test_f += f1_score(label, prediction, average='weighted')\n",
    "            n += 1\n",
    "    return test_l/n, test_p/n, test_r/n, test_f/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangqiqi/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train: loss 0.621037, precision 0.736780, recall 0.839416, f1 0.775558, time 0:00:00.034181\n",
      "          test: loss 0.621037,  precision 0.767120,  recall 0.858871,  f1 0.808494\n",
      "epoch 2, train: loss 0.618712, precision 0.736780, recall 0.839416, f1 0.775558, time 0:00:00.036902\n",
      "          test: loss 0.618712,  precision 0.767120,  recall 0.858871,  f1 0.808494\n",
      "epoch 3, train: loss 0.612661, precision 0.728588, recall 0.835766, f1 0.774240, time 0:00:00.035748\n",
      "          test: loss 0.612661,  precision 0.767120,  recall 0.858871,  f1 0.808494\n",
      "epoch 4, train: loss 0.612314, precision 0.736780, recall 0.839416, f1 0.775558, time 0:00:00.036307\n",
      "          test: loss 0.612314,  precision 0.767120,  recall 0.858871,  f1 0.808494\n",
      "epoch 5, train: loss 0.621237, precision 0.736780, recall 0.839416, f1 0.775558, time 0:00:00.039819\n",
      "          test: loss 0.621237,  precision 0.767120,  recall 0.858871,  f1 0.808494\n",
      "epoch 6, train: loss 0.615311, precision 0.736780, recall 0.839416, f1 0.775558, time 0:00:00.036863\n",
      "          test: loss 0.615311,  precision 0.767120,  recall 0.858871,  f1 0.808494\n",
      "epoch 7, train: loss 0.626577, precision 0.736780, recall 0.839416, f1 0.775558, time 0:00:00.043371\n",
      "          test: loss 0.626577,  precision 0.767120,  recall 0.858871,  f1 0.808494\n",
      "epoch 8, train: loss 0.622695, precision 0.736780, recall 0.839416, f1 0.775558, time 0:00:00.034688\n",
      "          test: loss 0.622695,  precision 0.767120,  recall 0.858871,  f1 0.808494\n",
      "epoch 9, train: loss 0.620292, precision 0.728588, recall 0.835766, f1 0.774240, time 0:00:00.035120\n",
      "          test: loss 0.620292,  precision 0.767120,  recall 0.858871,  f1 0.808494\n",
      "epoch 10, train: loss 0.618112, precision 0.728588, recall 0.835766, f1 0.774240, time 0:00:00.036026\n",
      "          test: loss 0.618112,  precision 0.767120,  recall 0.858871,  f1 0.808494\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 让Embedding层使用训练好的Word2Vec权重\n",
    "# model_word2vec = Word2Vec(train_feature_line, sg=1, min_count=1, size=128, window=5)\n",
    "# model_word2vec.save('word2vec_model.txt')\n",
    "w2v_model = Word2Vec.load('word2vec_model.txt')\n",
    "embedding_matrix = w2v_model.wv.vectors\n",
    "input_size = embedding_matrix.shape[0]   \n",
    "hidden_size = embedding_matrix.shape[1]  \n",
    "model = BiLSTM_CRF(input_size=input_size, hidden_size=hidden_size, output_size=9, embedding_vector=embedding_matrix).to(device)\n",
    "model.load_state_dict(torch.load('./model_2.pt'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-9, weight_decay=1e-5)  #\n",
    "\n",
    "train_loss, train_accuracy, train_precision, train_recall, train_f1 = [], [], [], [], []\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1 = [], [], [], [], []\n",
    "f1_min = 0.808494\n",
    "for epoch in range(args.num_epoch):\n",
    "    model.train()\n",
    "    train_l, train_p, train_r, train_f, n = 0.0, 0.0, 0.0, 0.0, 0\n",
    "    start = datetime.datetime.now()\n",
    "    for data_x, data_y, batch_seq_len in train_dataloader:\n",
    "        out = model(data_x.to(device),batch_seq_len).view(-1, 9)\n",
    "        label = [line.numpy().tolist() for line in data_y]\n",
    "        for line in label:\n",
    "            for i in range(data_x.shape[1]-len(line)):\n",
    "                line.append(line[len(line)-1])\n",
    "        label = torch.tensor(label).view(-1,1).squeeze(-1).to(device)\n",
    "        \n",
    "        out, label = processing_len(out, label,batch_seq_len)\n",
    "        \n",
    "        loss = loss_func(out, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prediction = out.argmax(dim=1).data.cpu().numpy()\n",
    "        label = label.data.cpu().numpy()\n",
    "        train_l += loss.item()\n",
    "        train_p += precision_score(label, prediction, average='weighted')\n",
    "        train_r += recall_score(label, prediction, average='weighted')\n",
    "        train_f += f1_score(label, prediction, average='weighted')\n",
    "        n += 1\n",
    "    #训练集评价指标\n",
    "    train_loss.append(train_l/n)\n",
    "    train_precision.append(train_p/n)\n",
    "    train_recall.append(train_r/n)\n",
    "    train_f1.append(train_f/n)\n",
    "    #测试集评价指标\n",
    "    test_l, test_p, test_r, test_f = test_evaluate(model, test_dataloader, args.batch_size)\n",
    "    test_loss.append(test_l)\n",
    "    test_precision.append(test_p)\n",
    "    test_recall.append(test_r)\n",
    "    test_f1.append(test_f)\n",
    "    end = datetime.datetime.now()\n",
    "    print('epoch %d, train: loss %f, precision %f, recall %f, f1 %f, time %s'% \n",
    "          (epoch+1, train_loss[epoch], train_precision[epoch], train_recall[epoch], train_f1[epoch], end-start))\n",
    "    print('          test: loss %f,  precision %f,  recall %f,  f1 %f'% \n",
    "          (test_loss[epoch], test_precision[epoch], test_recall[epoch], test_f1[epoch]))\n",
    "#     if test_f1[epoch] > f1_min:\n",
    "#         f1_min = test_f1[epoch]\n",
    "#         torch.save(model.state_dict(), './model_3.pt')\n",
    "#         print(\"save model......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
