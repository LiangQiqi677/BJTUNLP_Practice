{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import argparse\n",
    "from gensim.models import Word2Vec\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=1024)\n",
    "parser.add_argument('--num_epoch', type=int, default=500)\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入训练集\n",
    "with open('./CoNLL2003_NER/train/seq.in', encoding='utf-8') as ftrain_feature:\n",
    "    train_feature_line = [line.strip() for line in ftrain_feature.readlines()][:10]\n",
    "with open('./CoNLL2003_NER/train/seq.out', encoding='utf-8') as ftrain_label:\n",
    "    train_label_line = [line.strip() for line in ftrain_label.readlines()][:10]\n",
    "#读入验证集\n",
    "with open('./CoNLL2003_NER/test/seq.in', encoding='utf-8') as ftest_feature:\n",
    "    test_feature_line = [line.strip() for line in ftest_feature.readlines()][:10]\n",
    "with open('./CoNLL2003_NER/test/seq.out', encoding='utf-8') as ftest_label:\n",
    "    test_label_line = [line.strip() for line in ftest_label.readlines()][:10]\n",
    "\n",
    "#转换大小写并用split分隔开存入列表\n",
    "train_feature_line = [line.lower().split(\" \") for line in train_feature_line]\n",
    "train_label_line = [line.split(\" \") for line in train_label_line]\n",
    "test_feature_line = [line.lower().split(\" \") for line in test_feature_line]\n",
    "test_label_line = [line.split(\" \") for line in test_label_line]\n",
    "\n",
    "#获得单词字典\n",
    "word_counter = []\n",
    "for line in train_feature_line:\n",
    "    word_counter.extend(line)\n",
    "word_counter = Counter(word_counter).most_common()                                          #len(counter):21009\n",
    "vocab = ['[UNK]','[PAD]'] + [word[0] for word in word_counter[:int(len(word_counter)*0.8)]] #UNK:低频词；PAD:填充词\n",
    "word2id = dict(zip(vocab,range(len(vocab))))                                                # word -> id\n",
    "id2word = {idx:word for idx,word in enumerate(vocab)}                                       # id -> word\n",
    "#获得标签字典\n",
    "label2id = {'O':0, 'B-LOC':1, 'B-PER':2, 'B-ORG':3, 'I-PER':4, 'I-ORG':5, 'B-MISC':6, 'I-LOC':7, 'I-MISC':8, 'START':9, 'STOP':10}\n",
    "\n",
    "#获得数据和标签序列\n",
    "train_feature = [[word2id[word] if word in word2id else 0 for word in line] for line in train_feature_line]\n",
    "train_label = [[label2id[word] for word in line] for line in train_label_line]\n",
    "test_feature = [[word2id[word] if word in word2id else 0 for word in line] for line in test_feature_line]\n",
    "test_label = [[label2id[word] for word in line] for line in test_label_line]\n",
    "\n",
    "#转成Tensor的形式\n",
    "train_feature = [torch.Tensor(line).long() for line in train_feature]\n",
    "train_label = [torch.Tensor(line).long() for line in train_label]\n",
    "test_feature = [torch.Tensor(line).long() for line in test_feature]\n",
    "test_label = [torch.Tensor(line).long() for line in test_label]\n",
    "\n",
    "def get_data(sample_features, sample_labels):\n",
    "    sample_data = []                                                    #为了能够将data放到DataLoader中\n",
    "    for i in range(len(sample_features)):\n",
    "        temp = []\n",
    "        temp.append(sample_features[i])\n",
    "        temp.append(sample_labels[i])\n",
    "        sample_data.append(temp)\n",
    "    return sample_data\n",
    "\n",
    "def collate_fn(sample_data):\n",
    "    sample_data.sort(key=lambda data: len(data[0]), reverse=True)                          #倒序排序\n",
    "    sample_features, sample_labels = [], []\n",
    "    for data in sample_data:\n",
    "        sample_features.append(data[0])\n",
    "        sample_labels.append(data[1])\n",
    "    data_length = [len(data[0]) for data in sample_data]                                   #取出所有data的长度             \n",
    "    sample_features = torch.nn.utils.rnn.pad_sequence(sample_features, batch_first=True, padding_value=word2id['[PAD]']) \n",
    "    return sample_features, sample_labels, data_length\n",
    "\n",
    "train_data = get_data(train_feature, train_label)\n",
    "test_data = get_data(test_feature, test_label)\n",
    "\n",
    "#处理非定长序列\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, args.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, args.batch_size, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_vector, label2id):\n",
    "        super(BiLSTM,self).__init__()\n",
    "        \n",
    "        # ============================ BiLSTM的系列参数 ============================ #\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_vector))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.bilstm = torch.nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=2, \n",
    "                                    batch_first=True, dropout=0.5, bidirectional=True)\n",
    "        self.out = torch.nn.Linear(2 * hidden_size,output_size)\n",
    "        \n",
    "        # ============================= CRF的系列参数 ============================= #\n",
    "        self.tagset_size = len(label2id)\n",
    "        self.tag_to_ix = label2id\n",
    "        #转移矩阵，transitions[i][j]表示从label j转移到label i的概率,虽然是随机生成的但是后面会迭代更新\n",
    "        self.transitions = torch.nn.Parameter(torch.randn(self.tagset_size,self.tagset_size))\n",
    "        #这两个语句强制执行了这样的约束条件：我们永远不会转移到开始标签，也永远不会从停止标签转移\n",
    "        self.transitions.data[self.tag_to_ix['START'], :] = -10000     #从任何标签转移到START_TAG不可能\n",
    "        self.transitions.data[:, self.tag_to_ix['STOP']] = -10000      #从STOP_TAG转移到任何标签不可能\n",
    "\n",
    "# ======================================= 模型前向传播 ======================================= #\n",
    "    def forward(self, x, batch_seq_len):\n",
    "        \n",
    "        lstm_feats = self.BiLSTM(x, batch_seq_len)\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats) #BiLSTM处理结果作为CRF的输入,输出为分数和预测的标签序列\n",
    "        return score, tag_seq\n",
    "\n",
    "# ======================================= BiLSTM部分 ======================================= #\n",
    "    def BiLSTM(self, x, batch_seq_len):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        h = torch.zeros(4, batch_size, self.hidden_size).to(x.device) \n",
    "        c = torch.zeros(4, batch_size, self.hidden_size).to(x.device) \n",
    "        \n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x,batch_seq_len, batch_first=True)\n",
    "        output, hidden = self.bilstm(x, (h, c))\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output,batch_first=True)\n",
    "        lstm_feats = self.out(output)     \n",
    "        return lstm_feats\n",
    "     \n",
    "# ======================================= CRF的decode部分 ======================================= #\n",
    "    def _viterbi_decode(self, feats):\n",
    "        \n",
    "        # 加入batch的信息\n",
    "        batch_path_score = []\n",
    "        batch_best_path = []\n",
    "        for batch in range(feats.shape[0]):\n",
    "            backpointers = []\n",
    "\n",
    "            init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)    #保证了一定是从START到其他标签\n",
    "            init_vvars[0][self.tag_to_ix['START']] = 0                            #START的位置变成0\n",
    "\n",
    "            #        我  是  梁  棋  棋\n",
    "            #        |   |   |   |   |         这部分对应 for next_tag in range(self.tagset_size):\n",
    "            # START->O ->O ->B ->I ->I ->STOP  这部分对应 self.transitions\n",
    "\n",
    "            #每个time_step都需要前一个time_step的分数，每个time_step 对应 for feat in feats:\n",
    "            #                                            分数score 对应 forward_var\n",
    "            #所以forward_var = (torch.cat(viterbivars_t) + feat)\n",
    "            \n",
    "            forward_var = init_vvars                           \n",
    "            for feat in feats[batch]:                                                    \n",
    "                bptrs_t = []                                                     \n",
    "                viterbivars_t = []                                                \n",
    "                for next_tag in range(self.tagset_size):                          \n",
    "                    next_tag_var = forward_var + self.transitions[next_tag]       \n",
    "                    best_tag_id = argmax(next_tag_var)                  \n",
    "                    bptrs_t.append(best_tag_id)                                   \n",
    "                    viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "                forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "                backpointers.append(bptrs_t)                                      \n",
    "\n",
    "            #   棋\n",
    "            #   |\n",
    "            #   I ->STOP 这部分对应 terminal_var\n",
    "            #因为没有字对应‘STOP’标签，所以只需要加上转移到STOP的分数即可，即self.transitions[self.tag_to_ix['STOP']]\n",
    "\n",
    "            terminal_var = forward_var + self.transitions[self.tag_to_ix['STOP']] # 其他标签到STOP_TAG的转移概率\n",
    "            best_tag_id = argmax(terminal_var)\n",
    "            path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "            # 根据动态规划，由最后的节点，向前选取最佳的路径\n",
    "            best_path = [best_tag_id]\n",
    "            for bptrs_t in reversed(backpointers):\n",
    "                best_tag_id = bptrs_t[best_tag_id]\n",
    "                best_path.append(best_tag_id)\n",
    "            start = best_path.pop()\n",
    "            assert start == self.tag_to_ix['START']\n",
    "            best_path.reverse()                                 # 把从后向前的路径正过来\n",
    "            \n",
    "            batch_path_score.append(path_score)\n",
    "            batch_best_path.append(best_path)\n",
    "        return batch_path_score, batch_best_path\n",
    "    \n",
    "# ======================================= 计算模型loss ======================================= #    \n",
    "    def neg_log_likelihood(self, sentence, tags, batch_seq_len):\n",
    "        \n",
    "        feats = self.BiLSTM(sentence, batch_seq_len)\n",
    "        # loss = log(∑ e^s(X,y)) - s(X,y) \n",
    "        forward_score = self._forward_alg(feats)           # loss的log部分的结果\n",
    "        gold_score = self._score_sentence(feats, tags)     # loss的S(X,y)部分的结果\n",
    "        return (forward_score - gold_score)\n",
    "\n",
    "# ======================================= 计算loss的log部分 ======================================= #     \n",
    "    def _forward_alg(self, feats):\n",
    "    # 关于log_sum_exp的具体解释： https://blog.csdn.net/Suan2014/article/details/89477037\n",
    "    \n",
    "        # 加入batch的信息\n",
    "        alpha = torch.zeros(1).to(device)\n",
    "        for batch in range(feats.shape[0]):        \n",
    "            init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)  #保证了一定是从START到其他标签\n",
    "            init_alphas[0][self.tag_to_ix['START']] = 0.                         #START的位置变成0\n",
    "\n",
    "            forward_var = init_alphas                                            #包装到一个变量里面以便自动反向传播\n",
    "\n",
    "            for feat in feats[batch]:\n",
    "                alphas_t = []  \n",
    "                for next_tag in range(self.tagset_size):\n",
    "                    emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                    trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                    next_tag_var = forward_var + trans_score + emit_score\n",
    "                    alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "                forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "            terminal_var = forward_var + self.transitions[self.tag_to_ix['STOP']]\n",
    "            alpha = alpha + log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "    \n",
    "# ======================================= 计算loss的S(X,y)部分 ======================================= #    \n",
    "    def _score_sentence(self, feats, tags):\n",
    "        \n",
    "        #_forward_alg   ：算一个最大可能路径，实际上可能不是真实标签转移的值\n",
    "        #_score_sentence：用真实标签转移的值计算\n",
    "        score = torch.zeros(1).to(device)\n",
    "        for batch in range(feats.shape[0]):\n",
    "            # 将START_TAG的标签３拼接到tag序列最前面\n",
    "            tags = torch.cat([torch.tensor([self.tag_to_ix['START']], dtype=torch.long).to(device), tags])\n",
    "            for i, feat in enumerate(feats[batch]):\n",
    "                score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "            score = score + self.transitions[self.tag_to_ix['STOP'], tags[-1]]\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "# 关于log_sum_exp的具体解释： https://blog.csdn.net/Suan2014/article/details/89477037\n",
    "def log_sum_exp(vec): \n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1]) \n",
    "    \n",
    "    #等同于torch.log(torch.sum(torch.exp(vec)))，防止e的指数导致计算机上溢\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "#通过batch_seq_len留下out和label中不重复的部分进行loss计算和指标计算\n",
    "def processing_len(out, label, batch_seq_len):\n",
    "    out_pred = out[:batch_seq_len[0]]\n",
    "    out_true = label[:batch_seq_len[0]]\n",
    "    for i in range(1,len(batch_seq_len)):\n",
    "        out_pred = torch.cat((out_pred,out[i*batch_seq_len[0]:i*batch_seq_len[0]+batch_seq_len[i]]),dim=0)\n",
    "        out_true = torch.cat((out_true,label[i*batch_seq_len[0]:i*batch_seq_len[0]+batch_seq_len[i]]),dim=0)\n",
    "\n",
    "    return out_pred, out_true\n",
    "    \n",
    "def test_evaluate(model, test_dataloader, batch_size):\n",
    "    test_l, test_p, test_r, test_f, n = 0.0, 0.0, 0.0, 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data_x, data_y, batch_seq_len in test_dataloader:\n",
    "            _, out = model(data_x.to(device),batch_seq_len)                      #out就是路径序列 [10, 40]\n",
    "\n",
    "            label = [line.numpy().tolist() for line in data_y]\n",
    "            for line in label:\n",
    "                for i in range(data_x.shape[1]-len(line)):\n",
    "                    line.append(line[len(line)-1])\n",
    "\n",
    "            label = torch.tensor(label).view(-1,1).squeeze(-1).to(device)        #torch.Size([274])\n",
    "            out = torch.tensor(out).view(-1,1).squeeze(-1).to(device)          #torch.Size([274])\n",
    "            out, label = processing_len(out, label, batch_seq_len)\n",
    "            \n",
    "            loss = model.neg_log_likelihood(data_x.to(device), label, batch_seq_len)\n",
    "            out = out.data.cpu().numpy()\n",
    "            label = label.data.cpu().numpy()\n",
    "            \n",
    "            test_l += loss.item()/data_x.shape[0]/data_x.shape[1]\n",
    "            test_p += precision_score(label, out, average='weighted')\n",
    "            test_r += recall_score(label, out, average='weighted')\n",
    "            test_f += f1_score(label, out, average='weighted')\n",
    "            n += 1\n",
    "    return test_l/n, test_p/n, test_r/n, test_f/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangqiqi/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train: loss 824.450469, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:22.479681\n",
      "          test: loss 785.080060,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 2, train: loss 824.335781, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:34.551093\n",
      "          test: loss 784.937202,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 3, train: loss 824.174922, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:20.850348\n",
      "          test: loss 784.719345,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 4, train: loss 823.933203, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:38.825323\n",
      "          test: loss 784.477902,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 5, train: loss 823.670781, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:15.544467\n",
      "          test: loss 784.270238,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 6, train: loss 823.452188, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:42.520953\n",
      "          test: loss 784.108631,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 7, train: loss 823.294297, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:13.300043\n",
      "          test: loss 783.908482,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 8, train: loss 823.087969, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:48.365249\n",
      "          test: loss 783.679836,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 9, train: loss 822.844766, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:26.625666\n",
      "          test: loss 783.471503,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 10, train: loss 822.628906, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:12.280202\n",
      "          test: loss 783.276711,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 11, train: loss 822.426797, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:29.438233\n",
      "          test: loss 783.089583,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 12, train: loss 822.235937, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:32.753462\n",
      "          test: loss 782.879613,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 13, train: loss 822.020234, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.996963\n",
      "          test: loss 782.664509,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 14, train: loss 821.797734, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.806063\n",
      "          test: loss 782.444271,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 15, train: loss 821.571406, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.797412\n",
      "          test: loss 782.230580,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 16, train: loss 821.352969, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.815496\n",
      "          test: loss 782.025372,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 17, train: loss 821.141719, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.810141\n",
      "          test: loss 781.819345,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 18, train: loss 820.927578, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.806478\n",
      "          test: loss 781.613914,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 19, train: loss 820.709609, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.815354\n",
      "          test: loss 781.401190,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 20, train: loss 820.481719, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.874651\n",
      "          test: loss 781.191964,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 21, train: loss 820.259141, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.881720\n",
      "          test: loss 780.986012,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 22, train: loss 820.040234, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.829435\n",
      "          test: loss 780.772545,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 23, train: loss 819.815937, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.824503\n",
      "          test: loss 780.562946,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 24, train: loss 819.593125, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.823628\n",
      "          test: loss 780.339955,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 25, train: loss 819.359453, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.821662\n",
      "          test: loss 780.119494,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 26, train: loss 819.130391, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.835654\n",
      "          test: loss 779.911682,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 27, train: loss 818.914297, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.810295\n",
      "          test: loss 779.694196,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 28, train: loss 818.686250, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.812618\n",
      "          test: loss 779.471280,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 29, train: loss 818.452266, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.821932\n",
      "          test: loss 779.249628,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 30, train: loss 818.218047, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.827582\n",
      "          test: loss 779.030134,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 31, train: loss 817.988203, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.811321\n",
      "          test: loss 778.810119,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 32, train: loss 817.757656, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.833181\n",
      "          test: loss 778.584598,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 33, train: loss 817.521328, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.889894\n",
      "          test: loss 778.365476,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 34, train: loss 817.292734, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.824924\n",
      "          test: loss 778.136310,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 35, train: loss 817.053828, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.809386\n",
      "          test: loss 777.907589,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 36, train: loss 816.815547, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.914034\n",
      "          test: loss 777.679539,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 37, train: loss 816.580469, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.860617\n",
      "          test: loss 777.452976,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 38, train: loss 816.344922, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.850491\n",
      "          test: loss 777.241220,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 39, train: loss 816.124766, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.821038\n",
      "          test: loss 777.013988,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 40, train: loss 815.884531, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.828468\n",
      "          test: loss 776.784896,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 41, train: loss 815.644063, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.900170\n",
      "          test: loss 776.557440,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 42, train: loss 815.405234, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.887049\n",
      "          test: loss 776.334152,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 43, train: loss 815.169609, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.811615\n",
      "          test: loss 776.102232,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 44, train: loss 814.925703, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.820971\n",
      "          test: loss 775.874256,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 45, train: loss 814.684766, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.886576\n",
      "          test: loss 775.638914,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 46, train: loss 814.439609, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.832072\n",
      "          test: loss 775.407068,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 47, train: loss 814.196406, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.819561\n",
      "          test: loss 775.174330,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 48, train: loss 813.954375, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.832650\n",
      "          test: loss 774.941146,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 49, train: loss 813.710234, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.837588\n",
      "          test: loss 774.707961,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 50, train: loss 813.466563, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.814065\n",
      "          test: loss 774.477753,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 51, train: loss 813.222500, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.824061\n",
      "          test: loss 774.243452,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 52, train: loss 812.977422, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.833541\n",
      "          test: loss 774.009747,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 53, train: loss 812.733594, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.814047\n",
      "          test: loss 773.774479,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 54, train: loss 812.487109, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.832386\n",
      "          test: loss 773.537277,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 55, train: loss 812.240625, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.838012\n",
      "          test: loss 773.307589,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 56, train: loss 811.999766, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.894575\n",
      "          test: loss 773.070089,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 57, train: loss 811.752031, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.817966\n",
      "          test: loss 772.833557,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 58, train: loss 811.502734, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.913527\n",
      "          test: loss 772.602753,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 59, train: loss 811.264141, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.805752\n",
      "          test: loss 772.369792,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 60, train: loss 811.018984, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.813358\n",
      "          test: loss 772.130655,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 61, train: loss 810.767578, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.813677\n",
      "          test: loss 771.890997,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 62, train: loss 810.516328, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.889582\n",
      "          test: loss 771.654167,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 63, train: loss 810.269375, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.819124\n",
      "          test: loss 771.414137,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 64, train: loss 810.019375, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.802376\n",
      "          test: loss 771.174926,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 65, train: loss 809.768359, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.813119\n",
      "          test: loss 770.933780,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 66, train: loss 809.515547, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.821738\n",
      "          test: loss 770.706324,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 67, train: loss 809.275937, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.810314\n",
      "          test: loss 770.465551,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 68, train: loss 809.023594, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.818951\n",
      "          test: loss 770.225298,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 69, train: loss 808.772109, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.832140\n",
      "          test: loss 769.983631,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 70, train: loss 808.518594, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.849519\n",
      "          test: loss 769.749702,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 71, train: loss 808.272422, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:04.016841\n",
      "          test: loss 769.507143,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 72, train: loss 808.019531, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.950574\n",
      "          test: loss 769.265402,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 73, train: loss 807.767656, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:26.341787\n",
      "          test: loss 769.019717,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 74, train: loss 807.509922, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:11.055703\n",
      "          test: loss 768.789509,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 75, train: loss 807.267734, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.802387\n",
      "          test: loss 768.544717,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 76, train: loss 807.013750, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.806305\n",
      "          test: loss 768.303274,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 77, train: loss 806.759922, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.803677\n",
      "          test: loss 768.059301,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 78, train: loss 806.505937, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.826943\n",
      "          test: loss 767.815179,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 79, train: loss 806.248594, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.815826\n",
      "          test: loss 767.572545,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 80, train: loss 805.996719, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.819688\n",
      "          test: loss 767.327009,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 81, train: loss 805.738906, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.832696\n",
      "          test: loss 767.086161,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 82, train: loss 805.485391, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.859303\n",
      "          test: loss 766.840923,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 83, train: loss 805.228281, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.850566\n",
      "          test: loss 766.593824,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 84, train: loss 804.970156, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.833901\n",
      "          test: loss 766.348512,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 85, train: loss 804.714453, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.893430\n",
      "          test: loss 766.111235,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 86, train: loss 804.466328, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.821337\n",
      "          test: loss 765.865774,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 87, train: loss 804.210078, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.812174\n",
      "          test: loss 765.618452,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 88, train: loss 803.950469, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.811941\n",
      "          test: loss 765.379836,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 89, train: loss 803.700234, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.999807\n",
      "          test: loss 765.138914,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 90, train: loss 803.445234, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.821197\n",
      "          test: loss 764.892857,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 91, train: loss 803.187813, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.814812\n",
      "          test: loss 764.647173,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 92, train: loss 802.931016, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.825365\n",
      "          test: loss 764.398661,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 93, train: loss 802.670938, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.828462\n",
      "          test: loss 764.149256,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 94, train: loss 802.411797, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.854290\n",
      "          test: loss 763.909747,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 95, train: loss 802.162187, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.881254\n",
      "          test: loss 763.659598,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 96, train: loss 801.900156, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.829438\n",
      "          test: loss 763.416146,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 97, train: loss 801.641719, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.821192\n",
      "          test: loss 763.169196,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 98, train: loss 801.384688, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.818666\n",
      "          test: loss 762.920833,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 99, train: loss 801.123750, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.853285\n",
      "          test: loss 762.678720,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n",
      "epoch 100, train: loss 800.875234, precision 0.698506, recall 0.835766, f1 0.760996, time 0:00:03.831107\n",
      "          test: loss 762.428125,  precision 0.723872,  recall 0.850806,  f1 0.782223\n",
      "save model......\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 让Embedding层使用训练好的Word2Vec权重\n",
    "# model_word2vec = Word2Vec(train_feature_line, sg=1, min_count=1, size=128, window=5)\n",
    "# model_word2vec.save('word2vec_model.txt')\n",
    "w2v_model = Word2Vec.load('word2vec_model.txt')\n",
    "embedding_matrix = w2v_model.wv.vectors\n",
    "input_size = embedding_matrix.shape[0]   \n",
    "hidden_size = embedding_matrix.shape[1]  \n",
    "model = BiLSTM(input_size, hidden_size, 11, embedding_matrix,label2id).to(device)\n",
    "model.load_state_dict(torch.load('./model_3.pt'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)  #, weight_decay=1e-5\n",
    "\n",
    "train_loss, train_accuracy, train_precision, train_recall, train_f1 = [], [], [], [], []\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1 = [], [], [], [], []\n",
    "f1_min = 0.782223\n",
    "loss_min = 785.145015\n",
    "for epoch in range(args.num_epoch):\n",
    "    model.train()\n",
    "    train_l, train_p, train_r, train_f, n = 0.0, 0.0, 0.0, 0.0, 0\n",
    "    start = datetime.datetime.now()\n",
    "    for data_x, data_y, batch_seq_len in train_dataloader:\n",
    "        _, out = model(data_x.to(device),batch_seq_len)                      #out就是路径序列 [10, 40]\n",
    "        \n",
    "        label = [line.numpy().tolist() for line in data_y]\n",
    "        for line in label:\n",
    "            for i in range(data_x.shape[1]-len(line)):\n",
    "                line.append(line[len(line)-1])\n",
    "        label = torch.tensor(label).view(-1,1).squeeze(-1).to(device)        #torch.Size([274])\n",
    "        out = torch.tensor(out).view(-1,1).squeeze(-1).to(device)            #torch.Size([274])\n",
    "        out, label = processing_len(out, label, batch_seq_len)\n",
    "       \n",
    "        loss = model.neg_log_likelihood(data_x.to(device), label, batch_seq_len)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        out = out.data.cpu().numpy()\n",
    "        label = label.data.cpu().numpy()\n",
    "        train_l += loss.item()/data_x.shape[0]/data_x.shape[1]\n",
    "        train_p += precision_score(label, out, average='weighted')\n",
    "        train_r += recall_score(label, out, average='weighted')\n",
    "        train_f += f1_score(label, out, average='weighted')\n",
    "        n += 1\n",
    "    #训练集评价指标\n",
    "    train_loss.append(train_l/n)\n",
    "    train_precision.append(train_p/n)\n",
    "    train_recall.append(train_r/n)\n",
    "    train_f1.append(train_f/n)\n",
    "    #测试集评价指标\n",
    "    test_l, test_p, test_r, test_f = test_evaluate(model, test_dataloader, args.batch_size)\n",
    "    test_loss.append(test_l)\n",
    "    test_precision.append(test_p)\n",
    "    test_recall.append(test_r)\n",
    "    test_f1.append(test_f)\n",
    "    end = datetime.datetime.now()\n",
    "    print('epoch %d, train: loss %f, precision %f, recall %f, f1 %f, time %s'% \n",
    "          (epoch+1, train_loss[epoch], train_precision[epoch], train_recall[epoch], train_f1[epoch], end-start))\n",
    "    print('          test: loss %f,  precision %f,  recall %f,  f1 %f'% \n",
    "          (test_loss[epoch], test_precision[epoch], test_recall[epoch], test_f1[epoch]))\n",
    "    if test_f1[epoch] > f1_min or test_loss[epoch] < loss_min:\n",
    "        f1_min = test_f1[epoch]\n",
    "        loss_min = test_loss[epoch]\n",
    "        torch.save(model.state_dict(), './model_3.pt')\n",
    "        print(\"save model......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
